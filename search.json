[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Algrebra Lineal y Optimización",
    "section": "",
    "text": "Este book contiene los apuntes personales del curso Algrebra lineal y optimización, del Master of Data Science, dela Universidad Aldolfo Ibáñez 2022.\n\n\nFamiliarizarse con conceptos de Álgebra Lineal y Optimización, y su uso en Data Science\n\n\n\n\n\n\nVectores, matrices, valores propios, descomposiciones,…\nSistemas de ecuaciones lineales, regresión, PageRank, reducción de dimensionalidad, compresión,…\n\n\n\n\n\nOptimización lineal y convexa, multiplicadores de Lagrange, descenso por el gradiente, …\nRegresión, SVM, redes neuronales,\n\n\n\n\n\nVideos: Esencia del Álgebra Lineal - 3Blue1Brown Español"
  },
  {
    "objectID": "vectores.html",
    "href": "vectores.html",
    "title": "1  Vectores",
    "section": "",
    "text": "Vectores:\n\nUn vector es un arreglo unidimensional de números reales\n\n\nSu notación según nos convenga, escribiremos el vector como una fila o una columna.\n\n\n\nNotación de Vectores\n\n\nCuando trabajamos con vectores de dimensión n decimos que estamos en el espacio vectorial \\mathbb{R}^n"
  },
  {
    "objectID": "vectores.html#operaciones-de-suma-sobre-vectores",
    "href": "vectores.html#operaciones-de-suma-sobre-vectores",
    "title": "1  Vectores",
    "section": "1.2 Operaciones de Suma sobre Vectores",
    "text": "1.2 Operaciones de Suma sobre Vectores\n\n\n\nSuma de Vectores\n\n\n\n1.2.1 Geométricamente\n  \n\n\n1.2.2 Propiedades\n\n\n\n\n\n\nAsociatividad:\n\n\n\n\\bar{x}  + (\\bar{y}  + \\bar{z}) = (\\bar{x}  + \\bar{y}  ) + \\bar{z}\n\n\n\n\n\n\n\n\nConmutatividad:\n\n\n\n\\bar{x} + \\bar{y} = \\bar{y}  +\\bar{x}\n\n\n\n\n\n\n\n\nElemento Neutro:\n\n\n\nElemento Neutro: existe un elemento neutro \\bar{0} tal que \\bar{x} + \\bar{0} = \\bar{x} para todo \\bar{c} \\in \\mathbb{R}^n\nEn nuestro caso \\bar{0} es el vector:\n\\bar{0}= \\begin{bmatrix}\n0\\\\ \\vdots  \n\\\\0\n\\end{bmatrix}"
  },
  {
    "objectID": "vectores.html#operación-de-ponderación-de-vectores",
    "href": "vectores.html#operación-de-ponderación-de-vectores",
    "title": "1  Vectores",
    "section": "1.3 Operación de Ponderación de Vectores",
    "text": "1.3 Operación de Ponderación de Vectores\n\n\n\nPonderación de Vectores\n\n\n\n1.3.1 Geométricamente\n  \n\n\n1.3.2 Propiedades de Ponderación de Vectores\n\n\n\n\n\n\nDistributividad::\n\n\n\n\\alpha ( \\bar{x} +\\bar{y}) = \\alpha \\bar{x} + \\alpha \\bar{y} \\\\\n(\\alpha + \\beta) \\bar{x} = \\alpha \\bar{x} + \\beta \\bar{x}\n\n\n\n\n\n\n\n\nElemento Neutro::\n\n\n\n1\\bar{x}=\\bar{x}\n\n\n\n\n\n\n\n\nCompatibilidad::\n\n\n\n(\\alpha + \\beta) \\bar{x} = \\alpha (\\beta \\bar{x})\n\n\n\n\n\n\n\n\nObservaciones:\n\n\n\n\nPara todo \\bar{x} \\in \\mathbb{R}^n, el vector −1\\bar{x} lo escribimos como \\bar{x}. Ahora podemos hablar de la “resta”:\n\n\\bar{x} − \\bar{y} = \\bar{x} + (\\bar{y})\n\nPara todo \\bar{x} \\in \\mathbb{R}^n, tenemos \\bar{x}-\\bar{x} =\\bar{0}\n0\\bar{x} = \\bar{0} para todo \\bar{x} \\in \\mathbb{R}^n"
  },
  {
    "objectID": "vectores.html#norma",
    "href": "vectores.html#norma",
    "title": "1  Vectores",
    "section": "1.4 Norma",
    "text": "1.4 Norma\n\n\n\nLa norma de un vector\n\n\n\\|\\bar{x}\\| \\sqrt{x_1^2+\\cdot\\cdot\\cdot+x_n^2}\n\n\n\n\n\n\nDesigualdad Triangular:\n\n\n\n\\|\\bar{x} + \\bar{y}\\| \\leq \\|\\bar{x} \\|+ \\|\\bar{y}\\| \n\n\n\n\n\n\n\n\n\nNote\n\n\n\n \\|\\alpha \\bar{x}\\| = |\\alpha |\\cdot\\|\\bar{x}\\|\n \\|\\bar{x} \\| = 0\\Leftrightarrow \\bar{x}= \\bar{0}"
  },
  {
    "objectID": "vectores.html#distancia-mathbbrn",
    "href": "vectores.html#distancia-mathbbrn",
    "title": "1  Vectores",
    "section": "1.5 Distancia \\mathbb{R}^n",
    "text": "1.5 Distancia \\mathbb{R}^n\nLa norma nos permite definir la distancia entre vectores en \\mathbb{R}^n\nLa distancia entre \\bar{x} e \\bar{y} = \\|\\bar{x}-\\bar{y}\\|\n\n\n\nLa distancia entre \\bar{x} e \\bar{y}"
  },
  {
    "objectID": "vectores.html#producto-punto-o-interno",
    "href": "vectores.html#producto-punto-o-interno",
    "title": "1  Vectores",
    "section": "1.6 Producto punto o interno",
    "text": "1.6 Producto punto o interno\n\n\n\nProducto punto o interno"
  },
  {
    "objectID": "vectores.html#ortogonalidad",
    "href": "vectores.html#ortogonalidad",
    "title": "1  Vectores",
    "section": "1.7 Ortogonalidad",
    "text": "1.7 Ortogonalidad\nLos vectores \\bar{x} e \\bar{y} en \\mathbb{R}^n son ortogonales si \\bar{x} \\cdot\\bar{y}=0\n\n\n\nOrtogonalidad de Vectores cuando \\bar{x} \\cdot\\bar{y}=0"
  },
  {
    "objectID": "vectores.html#subespacios-vectoriales",
    "href": "vectores.html#subespacios-vectoriales",
    "title": "1  Vectores",
    "section": "1.8 Subespacios Vectoriales",
    "text": "1.8 Subespacios Vectoriales\nUn conjunto de vectores S \\subseteq \\mathbb{R}^n es un subespacio vectorial de \\mathbb{R}^n si:\n\n\n1.8.1 Ejeplos de subespacios vectoriales"
  },
  {
    "objectID": "vectores.html#combinaciones-lineales",
    "href": "vectores.html#combinaciones-lineales",
    "title": "1  Vectores",
    "section": "1.9 Combinaciones Lineales",
    "text": "1.9 Combinaciones Lineales\nUna combinación lineal de vectores \\bar{x}_1, ..., \\bar{x}_m\\in \\mathbb{R}^n es un vector en la forma:\n\\alpha_1\\bar{x}_1+ \\cdot\\cdot\\cdot +\\alpha_m\\bar{x}_m\nDonde \\alpha_1+ \\cdot\\cdot\\cdot +\\alpha_m \\in \\mathbb{R}\n\nEjemplos de Combinación lineal"
  },
  {
    "objectID": "vectores.html#espacio-generado-span",
    "href": "vectores.html#espacio-generado-span",
    "title": "1  Vectores",
    "section": "1.10 Espacio generado (span)",
    "text": "1.10 Espacio generado (span)\nEl espacio generado por vectores \\bar{x}_1, ..., \\bar{x}_m\\in \\mathbb{R}^n es el conjunto de todas las combinaciones lineales de \\bar{x}_1, ..., \\bar{x}_m\nEn símbolos:\nspan(\\bar{x}_1, ..., \\bar{x}_m)=\\{\\alpha_1\\bar{x}_1+ \\cdot\\cdot\\cdot +\\alpha_m\\bar{x}_m \\ |\\  \\alpha_1, ...,\\alpha_{m} \\in \\mathbb{R}\\}\n\n\n\n\n\n\nObservación:\n\n\n\nspan(\\bar{x}_1, ..., \\bar{x}_m) siempre es un subespacio vectorial de \\mathbb{R}^n"
  },
  {
    "objectID": "vectores.html#independencia-lineal",
    "href": "vectores.html#independencia-lineal",
    "title": "1  Vectores",
    "section": "1.11 Independencia lineal",
    "text": "1.11 Independencia lineal"
  },
  {
    "objectID": "vectores.html#bases",
    "href": "vectores.html#bases",
    "title": "1  Vectores",
    "section": "1.12 Bases",
    "text": "1.12 Bases\n\nEjemplos de Base:\n\n\n1.12.1 Bases y Dimensión\nDos propiedades claves acerca de bases:\n\n\n\n\n\n\nTerorema:\n\n\n\n\nTodo subespacio vectorial S de \\mathbb{R}^n tiene una base\nTodas las bases de un subespacio vectorial S de \\mathbb{R}^n tienen el mismo tamaño\n\n\n\nLa última proposición nos permite definir la dimensión de un subespacio vectorial S de \\mathbb{R}^n como el tamaño de sus bases.\n\n\n\n\n\n\nObservación\n\n\n\n\nLa dimensión de S = \\{\\bar{0}\\} es 0 y la dimensión de S = \\mathbb{R}^n es n\n\n\n\n\n\n1.12.2 Bases como sistemas de coordenadas\nDebido a la siguiente proposición, una base siempre provee un sistema de coordenadas para el espacio.\n\n\n\n\n\n\nProposición:\n\n\n\nSi B es una base para un subespacio vectorial S de \\mathbb{R}^n , entonces todo vector \\bar{x}\\in S se puede escribir de manera única como una combinación lineal de los vectores de B\n\n\n\n\n\n1.12.3 Bases ortonormales\nUna base ortonormal B de un subespacio vectorial S de \\mathbb{R}^n es una base de S que satisface:\n\nLos vectores en B son ortogonales entre sí (par a par)\nLa norma de cada vector en B es 1\n\nEl “cambio de coordenada” a una base ortonormal es mucho más simple:\n\nUna propiedad clave es la siguiente:\n\n\n\n\n\n\nTerorema:\n\n\n\nTodo subespacio vectorial S de \\mathbb{R}^n tiene una base ortonormal\n\n\nDe hecho, existe un procedimiento para transformar una base arbitraria a otra base ortonormal de manera que el span es el mismo (proceso de Gram-Schmidt)\n\n\n1.12.4 Bases Canónicas\n\n\nDe hecho, son ortogonales y tienen norma 1 * La base canónica es una base ortonormal"
  },
  {
    "objectID": "vectores.html#referencias-del-cápitulo",
    "href": "vectores.html#referencias-del-cápitulo",
    "title": "1  Vectores",
    "section": "1.13 Referencias del Cápitulo",
    "text": "1.13 Referencias del Cápitulo"
  },
  {
    "objectID": "matrices.html",
    "href": "matrices.html",
    "title": "2  Matrices",
    "section": "",
    "text": "Una matriz es un arreglo bidimensional de números reales\n\nDecimos que la matriz es de n \\times m si el número de filas es n y el número de columnas es m\n Si n = m decimos que la matriz es cuadrada\nNotación:\nSi A es una matriz, entonces:\n\nA_{i,j}denota la entrada en la fila i-ésima y columna j-ésima de A\nA_{i,*}denota la fila i-ésima de A (como vector)\nA_{*,j}denota la columna j-ésima de A (como vector)\n\n\nRecuerde: Escribimos un vector como fila o columna según nos convenga"
  },
  {
    "objectID": "matrices.html#suma-de-matrices",
    "href": "matrices.html#suma-de-matrices",
    "title": "2  Matrices",
    "section": "2.2 Suma de matrices",
    "text": "2.2 Suma de matrices\n\n\n\n\n\n\n\nPropiedades:\n\n\n\n\nAsociatividad: $A + (B + C) = (A + B) + C $\nConmutatividad: A + B = B + A\nElemento neutro e inversos"
  },
  {
    "objectID": "matrices.html#ponderación-de-matrices",
    "href": "matrices.html#ponderación-de-matrices",
    "title": "2  Matrices",
    "section": "2.3 Ponderación de Matrices",
    "text": "2.3 Ponderación de Matrices\n\n\n\n\n\n\nPropiedades:\n\n\n\n\nDistributividad:\n\n\\gamma(A + B) = \\gamma A + \\gamma B\\\\ (\\gamma + \\delta)A = \\gamma A + \\delta A\n\nCompatibilidad:\n\n(\\gamma\\delta)A = \\gamma(\\delta A)"
  },
  {
    "objectID": "matrices.html#multiplicación-matriz-vector",
    "href": "matrices.html#multiplicación-matriz-vector",
    "title": "2  Matrices",
    "section": "2.4 Multiplicación matriz-vector",
    "text": "2.4 Multiplicación matriz-vector\n \nEs útil visualizar el producto matriz-vector (por la derecha) como una combinación lineal:\n\nEs decir: A\\bar{x} es la combinación lineal de las columnas de A según los ponderadores en \\bar{x}\n\nLo mismo para el producto matrix-vector por la izquierda:\n\nEs decir: \\bar{x}A es la combinación lineal de las filas de A según los ponderadores en \\bar{x}"
  },
  {
    "objectID": "matrices.html#multiplicación-matriz-matriz",
    "href": "matrices.html#multiplicación-matriz-matriz",
    "title": "2  Matrices",
    "section": "2.5 Multiplicación matriz-matriz",
    "text": "2.5 Multiplicación matriz-matriz\n\nEs decir:\n(AB)_{i,j} es el producto punto entre la fila i-ésima de A y la columna j-ésima de B\nTambién podemos ver el producto matriz-matriz como varios productos matriz-vector:"
  },
  {
    "objectID": "matrices.html#producto-externo",
    "href": "matrices.html#producto-externo",
    "title": "2  Matrices",
    "section": "2.6 Producto Externo",
    "text": "2.6 Producto Externo\n\nEs decir, \\bar{x} \\otimes \\bar{y} es la matriz de n × m que satisface (\\bar{x}\\otimes\\bar{y}_{i,j} = x_iy_j\n\n\n\n\n\n\n\nPropiedades:\n\n\n\n\n\n\nPodemos ver el producto matriz-matriz en función de productos externos:\n \n\n\n\n\n\n\nPropiedades Multiplicación Matriz-Matriz:"
  },
  {
    "objectID": "matrices.html#matrices-como-transformaciones-lineales",
    "href": "matrices.html#matrices-como-transformaciones-lineales",
    "title": "2  Matrices",
    "section": "2.7 Matrices como transformaciones lineales",
    "text": "2.7 Matrices como transformaciones lineales\n\nDos espacios fundamentales\n   \nDos Espacios fundamentales Ejemplos"
  },
  {
    "objectID": "matrices.html#rango-y-nulidad",
    "href": "matrices.html#rango-y-nulidad",
    "title": "2  Matrices",
    "section": "2.8 Rango y nulidad",
    "text": "2.8 Rango y nulidad\nPara una matriz A de n × m definimos su rango y su nulidad como:\nrank(A) = dimensión de Col(A)\nnullity(A) = dimensión de Null(A)\nRango y nulidad\n   \n\n\n\n\n\n\nTeorema Rango Nulidad\n\n\n\nSea A una matriz de n \\times m. Tenemos que:\nrank(A) + nullity(A) = m\n\n\n\n\n\n\n\n\npropiedades del rango"
  },
  {
    "objectID": "matrices.html#transpuesta",
    "href": "matrices.html#transpuesta",
    "title": "2  Matrices",
    "section": "2.9 Transpuesta",
    "text": "2.9 Transpuesta\n  \n\n2.9.1 Espacio fila\n \n\n\n2.9.2 Mas propiedades del Rango"
  },
  {
    "objectID": "matrices.html#espacio-nulo-izquierdo",
    "href": "matrices.html#espacio-nulo-izquierdo",
    "title": "2  Matrices",
    "section": "2.10 Espacio nulo izquierdo",
    "text": "2.10 Espacio nulo izquierdo"
  },
  {
    "objectID": "matrices.html#los-4-espacios-fundamentales",
    "href": "matrices.html#los-4-espacios-fundamentales",
    "title": "2  Matrices",
    "section": "2.11 Los 4 espacios fundamentales",
    "text": "2.11 Los 4 espacios fundamentales\n\n\n2.11.1 Espacios Ortogonales\n\nComplementos Ortogonales\n\n\n\n2.11.2 Inversa de una matriz\n\n¿Cuándo existe la inversa de una matriz?"
  },
  {
    "objectID": "calculo.html",
    "href": "calculo.html",
    "title": "3  Cálculo y Optimización",
    "section": "",
    "text": "Supongamos que tenemos una función en una variable f : \\mathbb{R} → \\mathbb{R}.\nLa derivada de f en un punto x \\in \\mathbb{R} es:\n\nLa razón de cambio de de f entorno a x\n\ncambio = \\frac{f(x+h)-f(x)}{h} (en este caso h < 0)\n\n\n\nLa derivada de f en un punto x \\in \\mathbb{R}\n\n\nFormalmente:\nf'(x)= \\lim_{h \\to 0}\\frac{f(x+h)-f(x)}{h}\nPendiente de la recta tangente al grafo de f en el punto f(x)\nLa derivada nos da la mejor aproximación lineal de f entorno a x\nf(x + h) ≈ f(x) + f'(x)h\nObservaciones:\n\nLa derivada f'(x) no siempre existe\nSi existe decimos que f es derivable en x\nSi f es derivable en x, entonces es continua en x\nSi f es derivable en todo x \\in \\mathbb{R}, decimos que f es derivable\n\n\n\n\nEjemplos de funciones derivables y no\n\n\n\n\n\n\n\nEjemplos de Derivadas y reglas de cálculo\n\n\nRegla de la cadena: Si f(x) = h(g(x)) entonces f'(x) = h'(g(x)) \\cdot g'(x)\nEjemplo 1:\n(2ln(x^3)+e^x x^2+7)'\n2\\frac{1}{x^3}3x^2+e^x x^2+e^x2x \\frac{6x^2}{x^3}+e^x x^2+2^x x \\frac{6x}{x}+e^x x^2+2^x x\nEjemplo 2:\nf(x)= (5x^4-6x^3+x)^5\nf'(x)= 5(5x^4-6x^3+x)^4 \\cdot (20x^3-18x^2+1)\nf'(x)= (5x^4-6x^3+x)^4 \\cdot (100x^3-90x^2+5)\n\n\n\nAhora tenemos una función f : \\mathbb{R}^n \\to \\mathbb{R} con n variables ¿ Cómo definimos la derivada entorno a \\bar{x}\\in \\mathbb{R}^n ? Igual que antes pero necesitamos fijar una dirección para movernos\n\nLa dirección estará dada por un vector \\bar{v} \\in \\mathbb{R}^n (y la recta que define)\n\nDefinimos la derivada direccional de f entorno a \\bar{x} en dirección \\bar{v} como:\n\\nabla f'(\\bar{x})= \\lim_{h \\to 0}\\frac{f(\\bar{x}+h\\bar{v})-f(\\bar{x})}{h}\n\n\n\nDerivada direccional\n\n\n\n\n\n\nDefinición General:\n\nUna derivada parcial de una función de varia variables es su derivada respecto a una de esas variables con las otras mandeniéndose constantes. Las derivadas parciales son útiles en el cálculo vectorial y geometría diferencial.\n\n\nEjemplo:\nf(x, y)=3x^2+5xy-7y^2+1 \\frac{\\partial f}{\\partial x}=6x+5y \\cdot 1-0+0\n\\frac{\\partial f}{\\partial x}=6x+5y Las y se tratan como constantes y al derivarlas es cero.\nLa derivada parcial respecto a y\n\\frac{\\partial f}{\\partial y}=0+5x \\cdot 1+7\\cdot 2y+0 \\frac{\\partial f}{\\partial y}=5x+14y\nSi f : \\mathbb{R}^n \\to \\mathbb{R} está dada por n variables, definimos la derivada parcial de f entorno a \\bar{x} = (x_1, ..., x_n) con respecto a la variables x_i como la derivada direccional con dirección el vector canónico \\bar{e}_i\nDe manera equivalente, podemos definir la derivada parcial como:\n\\frac{\\partial f}{\\partial x_i}(x_1, ..., x_n)= \\lim_{h \\to 0} \\frac{f(x_1, ..., x_i+h, ...,x_n)-f(x_1,...,x_i, ..., x_n)}{h}\nDefinimos el gradiente como \\nabla f : \\mathbb{R}^n \\to \\mathbb{R}^n\n\nEjercicios:\n\n\n\nEjercicios de Derivadas parciales"
  },
  {
    "objectID": "calculo.html#gradiente-propiedades",
    "href": "calculo.html#gradiente-propiedades",
    "title": "3  Cálculo y Optimización",
    "section": "3.2 Gradiente: Propiedades",
    "text": "3.2 Gradiente: Propiedades\n\n\n\\nabla: Derivada Direccional\n\nSe pretende encontrar la dirección con la derivada direccional\n\n\n\nderivada direccional\n\n\n\n3.2.1 Hessiano (matriz)\nEn matemática, la matriz hessiana de una función escalar o campo escalar f de n variables, es la matriz cuadrada de tamaño n \\times n, de las segundas derivadas parciales. (wikipedia)\nAplicación de la matriz hessiana:\n\nConcavidad/Convexidad\nMétodo para determinar el carácter de los puntos críticos (máximos, mínimos y puntos de inflexión o silla o de ensilladura)\n\n\n\n\nMatriz hessiana\n\n\n\n\n\nDirección de la función\n\n\nReferencias: khanacademy: Hessiano\n\n\n3.2.2 Derivada: varias variables a varias variables\nSea f : R^n \\to R^m una función dada por las funciones coordenadas f = (f_1,..., f_m), para funciones f_i : R^n \\to R\nDefinimos el Jacobiano de f entorno a \\bar{x} como la matriz de m \\times n:\n\n\n\nJacobiano\n\n\n\nObservación:\n\nNuevamente el Jacobiano nos da la mejor aproximación lineal entorno a \\bar{x}\n\n\nf ( \\bar{x} + \\bar{h}) \\approx f ( \\bar{x}) + J_f ( \\bar{x} ) \\bar{h}\n\n\n\nRepresentación de la aproximación\n\n\n\nObservación:\n\nEl Hessiano de una función f : \\mathbb{R}^n \\to \\mathbb{R} es el Jacobiano del gradiente \\nabla f\n\nJacobiano:\n\nEn cálculo vectorial, la matriz Jacobiana de una función vectorial de varias variables es la matriz cuyos elementos son las derivadas parciales de primer orden de dicha función. Wikipedia\n\n\nReferencias: khanacademy: Jacobiano\nEjercicios"
  },
  {
    "objectID": "calculo.html#puntos-estacionarios",
    "href": "calculo.html#puntos-estacionarios",
    "title": "3  Cálculo y Optimización",
    "section": "3.3 Puntos Estacionarios",
    "text": "3.3 Puntos Estacionarios\nSupongamos que f : \\mathbb{R} \\to \\mathbb{R} es derivable. Decimos que x \\in \\mathbb{R} es un punto estacionario si f'(x) = 0. Un punto estacionario puede ser:\n\nMaximizador global\n\n\n\n\nFunción gráfica: Maximizador global\n\n\n\nMaximizador local\n\n\n\n\nFunción gráfica: Maximizador local\n\n\n\nMinimizador global\n\n\n\n\nFunción gráfica: Minimizador global\n\n\n\nMinimizador local\n\n\n\n\nFunción gráfica: Minimizador local\n\n\n\nPunto Silla\n\n\n\n\nFunción gráfica: Punto Silla\n\n\nPodemos diferenciar entre un maximizador y un minimizador con f':\nSi f′ cambia de signo en x de + a - \\to x es maximizador (local o global)\n\n\n\nSi f′ cambia de signo en x de + a - \\to x es maximizador (local o global)l)\n\n\nSi f′ cambia de signo en x de - a + \\to x es minimizador (local o global)\n\n\n\nSi f' cambia de signo en x de - a + \\to x es minimizador (local o global)\n\n\nPodemos diferenciar entre un maximizador y un minimizador usando la segunda derivada f''(x) si existe:\n$f′′(x) < 0 ⇒ x $ es maximizador (local o global)\n$f′′(x) > 0 ⇒ x $ es minimizador (local o global)\n\n\n\nDiferenciar entre un maximizador y un minimizador\n\n\nEjercicios\n\n¿Qué pasa con f′′(x) para el caso de punto silla?\nR: Si\n\n¿Es cierto que si f′′(x) = 0 entonces el punto estacionario es silla?\nR: No siempre"
  },
  {
    "objectID": "calculo.html#convexidad-y-concavidad",
    "href": "calculo.html#convexidad-y-concavidad",
    "title": "3  Cálculo y Optimización",
    "section": "3.4 Convexidad y concavidad",
    "text": "3.4 Convexidad y concavidad\nDecimos que una función f : \\mathbb{R} \\to \\mathbb{R} es convexa si para todo x, y \\in \\mathbb{R} y a \\in (0,1)\nf(ax + (1 − a)y) ≤ af(x) + (1 − a)f(y)\n\nf es estrictamente convexa si la desigualdad anterior es estricta <\n$f$ es cóncava si se cumple lo de arriba pero con ≥ en vez de ≤,\ny es estrictamente cóncava si se cumple con >\n\n\n\n\nFunción estricatamente convexa\n\n\nEjemplo: - Toda función lineal f(x) = cx + d es convexa y cóncava (se cumple la desigualdad con =) - f(x) = x^2 es estrictamente convexa\nObservaciones (análogo para concavidad):\n\nSi f tiene segunda derivada f′′ entonces f es convexa ⇔ f'' ≥ 0\nSi f tiene derivada f′ y es convexa entonces\nf′(x) = 0 ⇔ x es un minimizador global para f\nSi f es además estrictamente convexa entonces hay un único minimizador (el cual es global)\n\nEjercicios\n\nSuponga f1 y f2 son funciones convexas de \\mathbb{R} a \\mathbb{R}, ¿es cierto que la función min{f1, f2} es convexa?\n\n\n\n\nSolución al ejercicio a)\n\n\n\n¿Es cierto que la función max{f1, f2} es convexa?\n\n\n\n\nSolución al ejercicio b)\n\n\nPuntos estacionarios de $f : $^n \\mathbb{R}\nComo en el caso de una variable, tenemos puntos estacionarios donde\n\\nabla f(\\bar{x})= \\bar{0}\nLos puntos estacionarios pueden ser minimizadores/maximizadores locales/globales o puntos sillas…\n\n3.4.1 Convexidad: caso varias variables\nPodemos definir convexidad igual que antes:\nDecimos que una función f : \\mathbb{R} \\to \\mathbb{R} es convexa si para todo x, y \\in \\mathbb{R} y a \\in (0,1)\nf(ax + (1 − a)y) ≤ af(x) + (1 − a)f(y)\n\nf es estrictamente convexa si la desigualdad anterior es estricta <\n$f$ es cóncava si se cumple lo de arriba pero con ≥ en vez de ≤,\ny es estrictamente cóncava si se cumple con >\n\n\nSeguimos teniendo las buenas propiedades que antes:\n\nSi f : \\mathbb{R}^n \\to \\mathbb{R} tiene el Hessiano bien definido entonces: Hf es semidefinida positiva ⇔ f es convexa\nSi f : \\mathbb{R}^n \\to \\mathbb{R}es derivable en \\bar{x} y convexa entonces \\nabla f(\\bar{x}) = \\bar{0} ⇔ \\bar{x} \\in R^n es un minimizador global para f\nSi f además es estrictamente convexa, existe un único minimizador (el cual es global)\n\n\nRecordar:\n\nUna matriz simétrica A de n × n es semidefinida positiva si\n\n\n\\bar{x}^T A\\bar{x} ≥ 0 \\  \\forall\\  \\bar{x} \\in R^n\n\nOjo:\n\nel Hessiano (si existe) siempre es una matriz simétrica\n\n\nEjemplo: cuadrados mínimos\nSea A una matriz de n × m y \\bar{b} \\in R^n\nProblema de cuadrados mínimos: Buscar \\bar{x} \\in \\mathbb{R}^m tal que ∥A\\bar{x} −\\bar{b}∥^2 sea mínimo\nObservación: La función n ∥A\\bar{x} −\\bar{b}∥^2 es convexa\nf: \\mathbb{R}^m \\to \\mathbb{R}\n\\bar{x} \\to ||A\\bar{x}-\\bar{b}||^2=(A_{1,*}\\cdot\\bar{x}-b_1+..., A_{n,*}\\cdot\\bar{x}-b_n)^2\n\\frac{\\partial f}{\\partial x_i}= 2(A_{1,*}\\cdot\\bar{x}-b_1)A_{1, i}+...+2(A_{n,*}\\cdot\\bar{x}-b_n)A_{n, i}\n= 2(A\\bar{x}-\\bar{b})\\cdot A_{*,i}\n\\nabla f= 2A^T(A\\bar{x}-\\bar{b})\nLa ecuación \\nabla f(\\bar{x})=\\bar{0} nos da:\n2A^T (A\\bar{x}-\\bar{b})=\\bar{0} \\to A^T A\\bar{x}=A^T\\bar{b}\nSi 2A^T es invertible es convexa"
  },
  {
    "objectID": "iterativos.html",
    "href": "iterativos.html",
    "title": "4  Métodos Iterativos",
    "section": "",
    "text": "Hasta el momento, para minimizar hemos aprovechado convexidad, y encontrado una forma explícita o cerrada de la solución\nCuadrados mínimos y regresión lineal, es uno de los pocos casos en donde uno puede hacer esto. En general no es posible…\nEn el caso que no podemos encontrar una solución explicita o ni siquiera tenemos convexidad, una técnica clásica es utilizar métodos iterativos\nLos métodos iterativos pueden ser útiles también cuando tenemos una solución explícita (cuadrados mínimos/regresión lineal), ya que calcular esta solución puede ser muy caro (por ejemplo invertir una matriz muy grande). Los métodos iterativos ofrecen una solución aproximada de manera más rápida"
  },
  {
    "objectID": "iterativos.html#métodos-iterativos",
    "href": "iterativos.html#métodos-iterativos",
    "title": "4  Métodos Iterativos",
    "section": "4.2 Métodos Iterativos",
    "text": "4.2 Métodos Iterativos\nSupongamos que queremos minimizar la función f : \\mathbb{R}^n → \\mathbb{R}\nIdea de métodos iterativos:\n\nComenzamos en un vector x_0 \\in R^n\nEn cada iteración actualizamos el vector, según cierta regla (la idea es que el nuevo vector tenga un menor valor de f)\nNos detenemos según un criterio de parada\n\nNúmero máximo de iteraciones\nLa mejora en f es menor que cierta tolerancia\n\n\nVeremos primero dos métodos:\n\nDescenso por el gradiente (primer orden)\nMétodo de Newton (segundo orden)"
  },
  {
    "objectID": "iterativos.html#descenso-por-el-gradiente",
    "href": "iterativos.html#descenso-por-el-gradiente",
    "title": "4  Métodos Iterativos",
    "section": "4.3 Descenso por el gradiente",
    "text": "4.3 Descenso por el gradiente\nSupongamos que queremos minimizar la función f : \\mathbb{R}^n → \\mathbb{R}\nRecordar que la dirección contraria al gradiente, es decir, −\\nabla f, es la dirección de máximo descenso (\\|\\nabla f\\| es la magnitud del máximo descenso)\nDescenso por el gradiente:\n\nComenzamos en un vector \\bar{x}_0 \\in R^n\nEn cada iteración, actualizamos el vector según la regla:\n\n\\bar{x}_{k+1}\\leftarrow \\bar{x}_k- \\eta \\nabla f(\\bar{x}_k)\nDonde \\eta es la tasa de aprendizaje\n\nNos detenemos según el criterio de parada.\n\n\n\n\nEjemplo del Descenso del gradiente"
  },
  {
    "objectID": "iterativos.html#método-de-newton",
    "href": "iterativos.html#método-de-newton",
    "title": "4  Métodos Iterativos",
    "section": "4.4 Método de Newton",
    "text": "4.4 Método de Newton\n\n4.4.1 Encontrar Raíces de funciones: una variable\nEl método de Newton es un método iterativo para encontrar raíces de funciones (puntos donde la función es 0)\nIdea para encontrar raíces de f : \\mathbb{R} → \\mathbb{R} (caso una variable):\n\nEn cada iteración tendremos un punto x_k \\in \\mathbb{R} donde f(x_k) \\neq 0\nNos gustaría movernos a un punto x_k + h tal que f(x_k + h) = 0\nPodemos usar una aproximación lineal para f(x_k + h) = 0\nLa mejor aproximación lineal entorno a x_k está dada por la derivada:\n\nf(x_k + h) \\approx f'(x_k)h + f(x_k)\n\n\n\nDerivada de f(x_k)\n\n\n\nResolvemos en vez la ecuación:\n\nf'(x_k)h + f(x_k) = 0 \\qquad\\Rightarrow \\qquad h=\\frac{f(x_k)}{f'(x_k)}\n\n\n\nRepresentación gráfica de h\n\n\nLuego nos movemos al punto x_k- \\frac{f(x_k)}{f'(x_k)}\n\nComenzamos con un x_0 \\in \\mathbb{R}\nEn cada iteración, actualizamos el punto según la regla:\n\nx_{k+1}\\leftarrow x_k- \\frac{f(x_k)}{f'(x_k)} - Nos detenemos según un criterio de parada\n\n\n4.4.2 Minimizar una función: una variable\nSupongamos ahora que queremos minimizar una función f : \\mathbb{R} → \\mathbb{R} en una variable.\nPodemos usar Newton para encontrar una raíz de la derivada f′\nPara minimizar f : \\mathbb{R} → \\mathbb{R} (caso una variable):\n\nComenzamos con un x_0 \\in \\mathbb{R}\nEn cada iteración, actualizamos el punto según la regla:\n\nx_{k+1}\\leftarrow x_k- \\frac{f'(x_k)}{f''(x_k)}\n\nNos detenemos según un criterio de parada\n\n\n\n\nTasa de Aprendizaje dependiendo de la zona\n\n\n\n\n4.4.3 Minimizar una función: varias variable\nPara minimizar una función f : \\mathbb{R}^n \\rightarrow \\mathbb{R} (caso varias variables):\n\nComenzamos con un vector x ̄0 ∈ R\nEn cada iteración, actualizamos el vector según la regla:\n\nx_{k+1} \\leftarrow x_k−H_f(x_k)^{−1} \\nabla f(x_k) \n\nNos detenemos según un criterio de parada\n\nObservaciones: Necesitamos que la inversa del Hessiano exista"
  },
  {
    "objectID": "iterativos.html#método-de-newton-vs-descenso-por-el-gradiente",
    "href": "iterativos.html#método-de-newton-vs-descenso-por-el-gradiente",
    "title": "4  Métodos Iterativos",
    "section": "4.5 Método de Newton vs Descenso por el gradiente",
    "text": "4.5 Método de Newton vs Descenso por el gradiente\nMétodo de Newton:\n\nEn principio, no necesitamos ningún parámetro extra (por ej: tasa de aprendizaje)\nEn general mejor convergencia ya que utiliza la segunda derivada\nNecesita más supuestos: el Hessiano debe existir y ser invertible\nPuede ser más costoso computacionalmente ya que hay que calcular H_f^{−1}\n\nDescenso por el gradiente:\n\nNecesitamos escoger la tasa de aprendizaje\nMás simple: sólo usamos la primera derivada\nPuede ser más eficiente computacionalmente para datos grandes (muchas variables)"
  },
  {
    "objectID": "iterativos.html#aplicaciones-en-machine-learning",
    "href": "iterativos.html#aplicaciones-en-machine-learning",
    "title": "4  Métodos Iterativos",
    "section": "4.6 Aplicaciones en Machine Learning",
    "text": "4.6 Aplicaciones en Machine Learning\nAlgunos problemas que se pueden resuelven con métodos iterativos, y en particular, con Descenso del Gradiente:\n\nRegresión Lineal\nRegresión Logística (simple o multinomial)\nPerceptrón\nRedes Neuronales Feed-Forward (o Perceptron Multicapa)\n…\n\n\n4.6.1 Regresión Lineal\nDatos {(\\bar{x}_i, y_i)_{i=n}^n} donde cada x_i pertenece a \\mathbb{R}^m e y_i \\in \\mathbb{R}. y_i indica la respuesta correcta de \\bar{x}_i\nPara un dato nuevo \\bar{x} \\in \\mathbb{R}^m queremos predecir su valor de y\nRegresión Lineal:\nAprendemos la función de la forma:\n\\hat{y} = c_0 + c_1x_1 + \\cdots + c_m x_m Buscamos parámetros \\bar{c} que minimicen:\nLoss(\\bar{c})=\\frac{1}{n}\\sum_{i=1}^{n} (y_i-\\hat{y}_i)^2\nEjercicio: ¿Cómo se ve la regla de actualización de Descenso por el Gradiente?\nCaso univariado\nLoss(\\bar{c})= \\frac{1}{2}(y-\\hat{y})^2=(y-c_0-c_1x_1+\\cdots+c_mx_m)^2\nc_k\\leftarrow c_k+n2(y-\\hat{y})\\cdot x_k\nCaso multivariado\nc_k\\leftarrow c_k+y\\frac{2}{n}\\sum_{i=1}^{n} (y_i-\\hat{y}_i)\\cdot x_{ik}\n\n\n4.6.2 Regresión Logística\nUtilizado para clasificación binaria (versión simple) o múltiple (versión multinomial)\nClasificación binaria:\nDatos {(\\bar{x}_i, y_i)_{i=n}^n} donde cada x_i pertenece a \\mathbb{R}^m e y_i \\in {0,1}. y_i indica la respuesta correcta de \\bar{x}_i\nPara un dato nuevo \\bar{x} \\in \\mathbb{R}^m queremos predecir su clase, en este caso, la probabilidad de que y= 1\nRegresión Logísticas Simple\nAprendemos una función de la forma:\n\\hat{y} = sigmoid(u)\\frac{1}{1+e^{-u}}\n\n\n\nFunción Sigmoide\n\n\nu = c_0 + c_1x_1 + \\cdots + c_m x_m\nRegresión Logística simple: función de error\n¿Qué función de error minimizamos en este caso?\n\n¿Error cuadrático?\n\nMinimizamos\\; \\frac{1}{n} \\sum_{i=1}^n (y_i-\\hat{y}_i)^2\nEsta función no es convexa Esta función si es convexa\n\n\n\nFunción Convexa\n\n\n\nEjercicio:\n\nDemuestre que la regla de actualización para Descenso por el Gradiente se ve igual que la de Regresión Lineal\n\n\n\n\n\nDescenso por el Gradiente se ve igual que la de Regresión Lineal\n\n\nRegresión Logística multinomial\nClasificación múltiple (más de dos clases):\nDatos {(\\bar{x}_i, y_i)_{i=n}^n} donde cada x_i pertenece a \\mathbb{R}^m e y_i \\in \\mathbb{R}^c. y_i indica la clase correcta de \\bar{x}_i (es decir, \\bar{y}_i le asigna probabilidad de 1 al clase correcta)\nPara un dato nuevo \\bar{x} \\in \\mathbb{R}^m queremos predecir una distribución de clases de probabilidad sobra las C clases.\nAprendemos una función de la forma (softmax):\n\\hat{y} = softmax(u_1,\\dots,u_C) = \\left(\\frac{e^{u_1}}{\\sum_{k=1}^{C} e^{u_k}}, \\dots, \\frac{e^{u_C}}{\\sum_{k=1}^{C} e^{u_k}}\\right)\nu_k = c_{k0} + c_{k1}x_1 + \\dots + c_{km}x_m\nUtilizamos la entropía cruzada como error (para cada dato (\\bar{x}_i, \\bar{y}_i)):\n-\\sum_{k=1}^C =\\bar{y}_{ik}\\;log(\\hat{y}_{ik})"
  },
  {
    "objectID": "iterativos.html#perceptrón",
    "href": "iterativos.html#perceptrón",
    "title": "4  Métodos Iterativos",
    "section": "4.7 Perceptrón",
    "text": "4.7 Perceptrón\n\n\n\nRepresentación gráfica de un perceptrón\n\n\nu = x_1w_1 + x_2w_2 + \\dots + x_mw_m + b \\hat{y} = f(u)\n\nSi f = sigmoid tenemos el modelo de regresión logística simple\nSi f = id tenemos el modelo de regresión lineal\n\n\n4.7.1 Funciones de Activación\n\n\n\nFunciones de activación más comunes"
  },
  {
    "objectID": "iterativos.html#red-neuronal",
    "href": "iterativos.html#red-neuronal",
    "title": "4  Métodos Iterativos",
    "section": "4.8 Red Neuronal",
    "text": "4.8 Red Neuronal\n\n4.8.1 Red Neuronal caso simple\n\n\n\nRed Neuronal caso simple\n\n\nNotación matricial/vectorial:\n\n\\bar{h} = f(\\bar{x}W + b) (f se aplica coordenada a coordenada)\n\\hat{y} = g ( \\bar{h} \\bar{u}+c)\n\n\n\n4.8.2 Red Neuronal Feed-Forward\nFeedforward fully connected networks o Multilayer Perceptron (MLP):\n\n\n\nRed Neuronal Feed-Forward\n\n\n\nParámetros de la red: W^{(1)},\\bar{b}^{(1)},...,W^{(L)},\\bar{b}^{(L)},U,\\bar{c}\nHiperparámetros: cantidad de capas L, tamaño entrada y salida, tamaño capas, funciones de activación, función de salida.\n\n\n\n4.8.3 Red neuronal: caso clasificación\n\n\n\nRed neuronal: caso clasificación\n\n\n\nC es la cantidad de clases en donde estamos clasificando\n\\hat{y} es una distribución de probabilidad sobre las clases\nComúnmente la función de salida g es en C variables y se define como (softmax):\n\ng(a_1, \\dots, a_C)= softmax(a_1,\\dots,a_C) = \\left(\\frac{e^{a_1}}{\\sum_{k=1}^{C} e^{a_k}}, \\dots, \\frac{e^{a_C}}{\\sum_{k=1}^{C} e^{a_k}}\\right)\nCaso dos clases\n\nSi tenemos sólo 2 clases, podemos tener una sola salida (es decir, C = 1) y utilizar\n\ng(a) = sigmoid(a) - En cualquiera de los dos casos (2 clases o más) usamos como error la entropía cruzada\nCaso una sola neurona de salida\n\nTenemos una sola neurona de salida con función de activación id (i.e. sin función)\nUsamos el error cuadrático"
  },
  {
    "objectID": "iterativos.html#gradiente-y-entrenamiento-de-una-red",
    "href": "iterativos.html#gradiente-y-entrenamiento-de-una-red",
    "title": "4  Métodos Iterativos",
    "section": "4.9 Gradiente y Entrenamiento de una red",
    "text": "4.9 Gradiente y Entrenamiento de una red\nEs común usar Descenso por el Gradiente (y sus variantes) para encontrar los parámetros \\bar{\\Theta} = (W^{(1)}, \\bar{b}^{(1)}, \\dots, W^{(L)},\\bar{b}^{(L)}, U, \\bar{c}) de la red\nRegla de actualización: \\bar{\\theta} \\leftarrow \\bar{\\theta}-\\eta \\nabla loss(\\bar{\\theta})\nPodemos mirar esta regla coordenada a coordenada:\n\nPara cada parámetro \\theta actualizamos \\theta \\leftarrow \\theta-\\eta\\frac{\\partial loss}{\\partial\\theta}(\\bar{\\Theta})\n\n¿ Cómo calculamos \\frac{\\partial loss}{\\partial\\theta}(\\bar{\\Theta}) para el parámetro \\theta?\nRecordar que loss(\\bar{\\Theta)}= \\frac{1}{N}\\sum_{k = 1}^{N} error(\\hat{y}^{(k)}, \\bar{y}^{(k)})\nN es el tamaño del conjunto de entrenamiento.\n\\Rightarrow \\; \\frac{\\partial loss}{\\partial\\theta}(\\bar{\\Theta})= \\frac{1}{N}\\sum_{k = 1}^{N} \\frac{error(\\hat{y}^{(k)}, \\bar{y}^{(k)})}{\\partial\\theta}(\\bar{\\Theta})\nNos podemos enfocar en calcular \\frac{error(\\hat{y}^{(k)}, \\bar{y}^{(k)})}{\\partial\\theta}(\\bar{\\Theta}) para un dato genérico (\\bar{x}, \\bar{y})\nTenemos un dato fijo (\\bar{x}, \\bar{y}) y un punto \\bar{\\Theta} \\in \\mathbb{R}^d. Queremos calcular \\frac{error(\\hat{y}^{(k)}, \\bar{y}^{(k)})}{\\partial\\theta}(\\bar{\\Theta}) para todos los parámetros \\theta\nPodemos usar Backpropagation\n\nIdea backpropagation:\n\nRegla de la cadena + programación dinámica"
  },
  {
    "objectID": "iterativos.html#regla-de-la-cadena",
    "href": "iterativos.html#regla-de-la-cadena",
    "title": "4  Métodos Iterativos",
    "section": "4.10 Regla de la cadena",
    "text": "4.10 Regla de la cadena\n\n\n\nRegla de la cadena con una y varias variables\n\n\n\n\n\nEjemplo de la regla de la cadena\n\n\n\n4.10.1 Backpropagation\n\n\n\nEjemplo de Backpropagation, condiciones iniciales\n\n\n\n\n\nBackpropagation: fase forward\n\n\n\n\n\nBackpropagation: fase backward\n\n\n\n\n\nBackpropagation, nuevo vector para el descenso por gradiente\n\n\n\n\n\nBackpropagation: ejemplo donde el fase backward se necesita la regla de la cadena"
  },
  {
    "objectID": "gradient_desc.html",
    "href": "gradient_desc.html",
    "title": "5  Descenso del Gradiente",
    "section": "",
    "text": "Enunciado de Ejemplo: Muchas veces la función a minimizar corresponde a una suma o un promedio de funciones Por ejemplo aprendizaje supervisado:\nDatos {(\\bar{x}_i, y_i)_{i=n}^n} donde cada x_i pertenece a \\mathbb{R}^m e y_i es la respuesta correcta. e_i es el error de nuestro modelo sobre el dato (\\bar{x}_i, \\bar{y}_i)\nBuscamos los parámetros \\Theta que minimicen el error promedio sobre los datos, es decir:\nJ(\\Theta) = \\frac{1}{n} \\sum_{i =1}^n e_i\n\n\nDatos {(\\bar{x}_i, y_i)_{i=n}^n}, \\Theta \\in \\mathbb{R}^d parámetros, J(\\Theta) = \\frac{1}{n} \\sum_{i =1}^n e_i\n\nComenzamos en un vector \\Theta_0 \\in \\mathbb{R}^d\nEn cada iteración, actualizamos cada parámetro \\theta\\in \\Theta según la regla:\n\n\\theta_{k+1}\\leftarrow \\theta_k- \\eta \\frac{\\partial J}{\\partial \\theta} (\\Theta_k))\nDonde \\eta la tasa de aprendizaje\n\nNos detenemos según un criterio de parada\n\n¿Algún problema con este método?\n\nSi hay muchos datos (n es grande) puede ser costoso calcular J y sus derivadas\n\n\n\n\nDatos {(\\bar{x}_i, y_i)_{i=n}^n}, \\Theta \\in \\mathbb{R}^d parámetros, J(\\Theta) = \\frac{1}{n} \\sum_{i =1}^n e_i\n\nComenzamos en un vector \\Theta_0 \\in \\mathbb{R}^d\nEn cada iteración, escogemos un dato (\\bar{x}_i, \\bar{y}_i) y actualizamos cada parámetro \\theta\\in \\Theta según la regla:\n\n\\theta_{k+1}\\leftarrow \\theta_k - \\eta \\frac{\\partial e_i}{\\partial \\theta} (\\Theta_k))\nDonde \\eta la tasa de aprendizaje\n\nNos detenemos según un criterio de parada\n\n\n\n\nEjemplo gráfico del descenso de gradiente estocástico\n\n\n¿Algún problema con este método?\n\nMás eficiente\nComportamiento de convergencia más impredecible\n\n\n\n\nDatos {(\\bar{x}_i, y_i)_{i=n}^n}, \\Theta \\in \\mathbb{R}^d parámetros, J(\\Theta) = \\frac{1}{n} \\sum_{i =1}^n e_i\n\nComenzamos en un vector \\Theta_0 \\in \\mathbb{R}^d\nEn cada iteración, escogemos un mini-batch de datos \\{(\\bar{x}_i, \\bar{y}_i)_\\ell^B\\}=1 y actualizamos cada parámetro \\theta \\in \\Theta según la regla:\n\n\\theta_{k+1}\\leftarrow \\theta_k - \\eta \\frac{\\partial J'}{\\partial \\theta} (\\Theta_k))\nJ'(\\Theta)= \\frac{1}{n}\\sum_{\\ell=1}^B e_\\ell Donde \\eta la tasa de aprendizaje\n\nNos detenemos según un criterio de parada\n\n\n\n\nEjemplo gráfico del descenso del gradiente mini-batch\n\n\nDistintas formas de obtener mini-batch:\n\nOrdenar aleatoriamente los datos e ir extrayendo mini-batches en orden\n\nUna época se termina cuando visitamos todos los datos Es uno de los métodos más utilizados\n\n\n\nIdea:\nPara escoger la dirección a movernos desde un punto \\Theta_0 \\in \\mathbb{R}^d, no sólo miramos el gradiente actual, sino también cómo nos movimos en el pasado (historia de gradientes pasados)\n\n\n\nDescenso del gradiente sin momentum y con momentum\n\n\nRegla de actualización:\n\\bar{v}_k\\; \\leftarrow(1-\\beta)\\; \\nabla J(\\Theta_k)+ \\beta\\bar{v}_{k-1}\n(\\bar{v}_k es la dirección a movernos que considera el gradiente actual y la historia pasada; se asume \\bar{v}_{-1}=\\bar{0}).\n\\Theta_{k+1}\\; \\leftarrow \\Theta_k-\\eta\\bar{v}_k\nEn cada paso, \\bar{v}_k es el promedio exponencial móvil de los gradientes hasta el momento\n\nLe damos prioridad a los últimos gradientes. La prioridad decae exponencialmente\nUn valor común para \\beta es 0.9\n\n\\bar{v}_k\\;= (1-\\beta)\\;\\nabla J(\\Theta_k)+ \\beta (1-\\beta)\\;\\nabla J(\\Theta_{k-1})+ \\\\ \\beta^2 (1-\\beta)\\;\\nabla J(\\Theta_{k-1})+ \\dots+ \\beta^i (1-\\beta)\\;\\nabla J(\\Theta_{k-i})+\\dots+ \\\\\\beta^k (1-\\beta)\\;\\nabla J(\\Theta_{0}) (Si \\beta = 0.9 e i = 100, entonces \\beta^i (1 − \\beta) = 0.00000265613)\n\n\n\nRepresentación gráfica del efecto de aplicar momentun y no, mediante vectores\n\n\nEn cada paso, \\bar{v}_k es el promedio exponencial móvil de los gradientes hasta el momento - Le damos prioridad a los últimos gradientes. La prioridad decae exponencialmente - Un valor común para \\beta es 0.9\nRegla de actualización:\nv_k\\; \\leftarrow(1-\\beta)\\;\\frac{\\partial J}{\\partial \\theta}(\\Theta_k)+ \\beta v_{k-1}\n\\Theta_{k+1}\\; \\leftarrow \\Theta_k-\\eta v_k\nRegla de actualización (otra opción):\n\\bar{v}_k\\; \\leftarrow \\beta\\bar{v}_{k-1} -(1-\\beta)\\; \\nabla J(\\Theta_k)\nv_k\\; \\leftarrow  \\beta v_{k-1}-(1-\\beta)\\;\\frac{\\partial J}{\\partial \\theta}(\\Theta_k)\n\n\n\nIdea:\nEn vez de mirar los gradientes pasados y el gradiente en el punto actual, miramos los gradientes pasados y el gradiente en un punto futuro, según el momentum\n\n\n\nRepresentación gráfica del Descenso por el gradiente con momentum + Nesterov"
  },
  {
    "objectID": "gradient_desc.html#métodos-adaptativos-adagrad-rmsprop-adam",
    "href": "gradient_desc.html#métodos-adaptativos-adagrad-rmsprop-adam",
    "title": "5  Descenso del Gradiente",
    "section": "5.2 Métodos adaptativos: AdaGrad, RMSProp, Adam",
    "text": "5.2 Métodos adaptativos: AdaGrad, RMSProp, Adam\nIdea Queremos una tasa de aprendizaje adaptativa, que cambie en el tiempo\n\n5.2.1 AdaGrad\n\nIdea: Avanzar de manera uniforme en todas las dimensiones (parámetros)\nNormalizamos la tasa eta por la historia de los gradientes (sus cuadrados)\n\n\n\n\nRepresentación gráfica de AdaGrad\n\n\nRegla de actualización (por parámetro):\nr_k = \\leftarrow r_{k-1}+ \\frac{\\partial J}{\\partial\\theta}(\\Theta_k)^2\n(r_k almacena la suma de los cuadrados de todos los gradientes)\n\\theta_{k+1}  \\leftarrow \\theta_k - \\frac{\\eta}{\\sqrt{r_k}} \\cdot \\frac{\\partial J}{\\partial\\theta}(\\Theta_k)\n\n\n5.2.2 RMSProp:\n\nIdea: Avanzar de manera uniforme en todas las dimensiones (parámetros)\nNormalizamos la tasa \\eta por el promedio exponencial móvil de los cuadrados de los gradientes\n\nRegla de actualización (por parámetro):\ns_k = \\leftarrow \\beta s_{k-1}+ (1-\\beta)\\frac{\\partial J}{\\partial\\theta}(\\Theta_k)^2 \\theta_{k+1}  \\leftarrow \\theta_k - \\frac{\\eta}{\\sqrt{s_k}} \\cdot \\frac{\\partial J}{\\partial\\theta}(\\Theta_k)\n\n\n5.2.3 Adam:\nIdea: Combinar descenso con momentum y RMSProp\nRegla de Actualización\nv_k \\leftarrow \\beta_1 v_{k-1}- (1-\\beta_1)\\frac{\\partial J}{\\partial\\theta}(\\Theta_k)\ns_k  \\leftarrow \\beta_2 s_{k-1}+ (1-\\beta_2)\\frac{\\partial J}{\\partial\\theta}(\\Theta_k)^2 v'_k  \\leftarrow \\frac{v_k}{1-(\\beta_1)^k} s'_k  \\leftarrow \\frac{s_k}{1-(\\beta_2)^k}\n\\theta_{k+1}  \\leftarrow \\theta_k + \\frac{\\eta}{\\sqrt{s'_k}} \\cdot v'_k   \nTípicamente, \\beta1 = 0.9, \\beta2 = 0.999, \\eta = 0.001"
  },
  {
    "objectID": "gradient_desc.html#regularización",
    "href": "gradient_desc.html#regularización",
    "title": "5  Descenso del Gradiente",
    "section": "5.3 Regularización",
    "text": "5.3 Regularización\nIdea:\n\nAgregar un término de penalización para obtener soluciones “simples”\n\nEn Machine Learning:\n\nObtener modelos más simples\n\nParámetros más pequeños o menos parámetros (modelo sparse)\n\nModelos simples ayudan a evitar overfitting\n\n\n5.3.1 Regularización \\ell_2\nIdea: - Favorecer soluciones con parámetros pequeños\nEn vez de minimizar J(Θ), minimizamos:\n\\tilde{J}(\\Theta) = J(\\Theta)  + \\alpha\\|\\Theta\\|_2^2\nDonde |\\Theta\\|_2 = \\sqrt{\\sum_{i=1}^m \\theta_i^2} para \\Theta = (\\theta_1, \\dots, \\theta_m)\n\nEl término \\apha mide cuánto peso tiene la regularización\nEl término \\alpha\\|\\Theta\\|_2^2 es diferenciable luego podemos aplicar las variantes de descenso del gradiente\n\n(notar \\nabla\\alpha \\|\\Theta\\|_2^2= 2\\alpha\\Theta)\nCaso de Regresión Lineal minimizamos:\n\\tilde{J}=\\frac{1}{n}\\sum_{i=1}^{n} (\\Theta^T\\bar{x}_i- y_i)^2+ \\alpha\\|\\Theta\\|_2^2\nEsto se llama Ridge Regression\nRegla del gradiente:\n\\Theta \\leftarrow \\Theta-\\eta\\; \\left(\\frac{2}{n}X^T(X\\Theta-\\bar{y})+2\\alpha\\Theta\\right)\nForma Alternativa:\n\\Theta \\leftarrow \\Theta-\\eta\\; \\frac{2}{n}X^T(X\\Theta-\\bar{y})+2\\eta\\alpha\\Theta \\Theta \\leftarrow (1-2\\eta\\alpha)\\Theta-\\eta \\frac{2}{n}X^T(X\\Theta-\\bar{y}) Es decir, la regla normal con \\beta extra:\n\\Theta \\leftarrow \\beta \\Theta-\\eta\\; \\frac{2}{n}X^T(X\\Theta-\\bar{y})\n\n\n5.3.2 Regularización \\ell_1\nIdea:\n\nFavorecer soluciones con menos parámetros\n\nEn vez de minimizar J(Θ), minimizamos:\n\\tilde{J}(\\Theta) = J(\\Theta)  + \\alpha\\|\\Theta\\|_1\nDonde |\\Theta\\|_1 = \\sum_{i=1}^m |\\theta_i| para \\Theta = (\\theta_1, \\dots, \\theta_m)\n¿ Cuánto es \\nabla\\alpha\\|\\Theta\\|_1 ?\n$$=\n\\begin{cases}\n\n\n      1, & \\text{si } \\theta > 0 \\\\\n      -1, & \\text{si } \\theta < 0\n    \\end{cases}\n$$\nPara implementar descenso por el gradientes es común asumir:\n$$=\n\\begin{cases}\n\n\n1, & \\text{si } \\theta > 0 \\\\\n0, & \\text{si } \\theta = 0 \\\\\n-1, & \\text{si } \\theta < 0 \\end{cases}\n$$\n\\nabla\\alpha\\|\\Theta\\|_1= \\alpha \\text{ sing}(\\Theta)\n$$()=\n\\begin{cases}\n\n\n      1, & \\text{si } \\theta > 0 \\\\\n      0, & \\text{si } \\theta = 0 \\\\\n      -1, & \\text{si } \\theta < 0 \\end{cases}\n$$ En el caso de Regresión Lineal esto se llama Lasso Regression\n\n\n5.3.3 Ridge vs Lasso\n\n\n\nRepresentación Gráfica Ridge vs Lasso\n\n\n\n\n5.3.4 Combinando \\ell_1 y \\ell_2\nPodemos combinar las dos técnicas anteriores y minimizar:\n\\tilde{J}(\\Theta) = J(\\Theta)  + ra\\|\\Theta\\|_1 +(1-r)a\\|\\Theta\\|_2^2\ndonde $0 ≤ r ≤ 1 $es un parámetro\nEn el caso de Regresión Lineal esto se llama **Elastic Net^^"
  },
  {
    "objectID": "gradient_desc.html#referencias",
    "href": "gradient_desc.html#referencias",
    "title": "5  Descenso del Gradiente",
    "section": "5.4 Referencias",
    "text": "5.4 Referencias\nBook: Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow - Aurélien Géro\nVideo: Descenso de Gradiente. Cómo Aprenden las Redes Neuronales | Aprendizaje Profundo. Capítulo 2\nBook: Neural Networks and Deep Learning - Michael Nielsen"
  }
]